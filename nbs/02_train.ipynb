{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "omlzs2cvvvb",
   "metadata": {},
   "source": [
    "# Embedding Model Fine-tuning with Sentence Transformers\n",
    "\n",
    "[![Open In Colab](https://img.shields.io/badge/Open%20In-Colab-blue?style=for-the-badge&logo=google-colab)](https://colab.research.google.com/github/dnth/rag-datakit/blob/main/nbs/02_train.ipynb)\n",
    "[![Open In Kaggle](https://img.shields.io/badge/Open%20In-Kaggle-blue?style=for-the-badge&logo=kaggle)](https://kaggle.com/kernels/welcome?src=https://github.com/dnth/rag-datakit/blob/main/nbs/02_train.ipynb)\n",
    "\n",
    "This notebook demonstrates how to fine-tune an embedding model using the synthetic triplet data generated from Singapore SkillsFuture Framework job descriptions. We'll use the sentence-transformers library to train a model that can better understand job-related semantic similarity for improved retrieval and matching.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to set up sentence-transformers training pipeline\n",
    "- Configuring training arguments for embedding models\n",
    "- Using MultipleNegativesRankingLoss for triplet training\n",
    "- Monitoring training with Weights & Biases\n",
    "- Saving and publishing trained models\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install the required dependencies for embedding model training:\n",
    "\n",
    "```bash\n",
    "pip install sentence-transformers datasets wandb torch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jllnb3x8pe",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We begin by importing the essential libraries for training our embedding model:\n",
    "\n",
    "- **sentence_transformers**: The core library providing tools for training and evaluating sentence transformers\n",
    "- **datasets**: Hugging Face's library for loading and managing our training dataset\n",
    "- **wandb**: Weights & Biases for experiment tracking and visualization of training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52da3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cb44d",
   "metadata": {},
   "source": [
    "## Dataset Loading and Inspection\n",
    "\n",
    "We load the preprocessed training dataset with train/validation splits that were created in the previous notebook. This dataset contains triplets specifically formatted for embedding model training using contrastive learning approaches.\n",
    "\n",
    "**Dataset source**: `dnth/ssf-train-valid`  \n",
    "**Structure**:\n",
    "- **anchor**: Original job descriptions from the SkillsFuture Framework\n",
    "- **positive**: Semantically similar paraphrases generated through synthetic data techniques\n",
    "- **negative**: Semantically different job descriptions used as contrastive examples\n",
    "\n",
    "Let's examine the dataset structure and review a sample triplet to understand the data format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a89eb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative'],\n",
       "        num_rows: 1508\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative'],\n",
       "        num_rows: 377\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dnth/ssf-train-valid\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab59509f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': 'A Principal Speech Therapy Educator is responsible for designing training activities as well as providing clinical and professional education and training. S/He also needs to manage quality of training and development activities. S/He may work in various settings such as public and private institutions, integrated and long-term care facilities as well as in the community setting. S/He may work as part of collaborative and interdisciplinary teams. S/He should be creative, insightful, practical and adaptable.',\n",
       " 'positive': 'Job opening for a Senior Speech Therapy Trainer focusing on developing educational programs and providing clinical training. The role involves ensuring high standards in training quality and may take place in diverse environments including hospitals, rehabilitation centers, and community outreach programs. Collaboration with multidisciplinary teams is essential, requiring creativity, adaptability, and practical skills.',\n",
       " 'negative': 'Seeking a Junior Occupational Health and Safety Coordinator to implement safety training programs and conduct health assessments in various workplace environments. The role includes managing compliance with safety regulations and promoting a culture of safety among employees. Candidates should possess strong analytical skills, attention to detail, and the ability to work effectively in teams.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['valid'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_wandb",
   "metadata": {},
   "source": [
    "## Initialize Weights & Biases and Model Configuration\n",
    "\n",
    "We initialize Weights & Biases for experiment tracking and define our base model and save path. We're using the `all-minilm-l6-v2` model as our starting point, which is a efficient, general-purpose sentence embedding model that provides a good balance between speed and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981a6573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdnth\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dnth/rag-datakit-finetunes/runs/qsq6z6xp' target=\"_blank\">all-minilm-l6</a></strong> to <a href='https://wandb.ai/dnth/rag-datakit-finetunes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dnth/rag-datakit-finetunes' target=\"_blank\">https://wandb.ai/dnth/rag-datakit-finetunes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dnth/rag-datakit-finetunes/runs/qsq6z6xp' target=\"_blank\">https://wandb.ai/dnth/rag-datakit-finetunes/runs/qsq6z6xp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dnth/rag-datakit-finetunes/runs/qsq6z6xp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7eb5f6236fc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "model_id = \"sentence-transformers/all-minilm-l6-v2\"\n",
    "save_model_path = \"./models/all-minilm-l6-v2\"\n",
    "\n",
    "# wandb.login()\n",
    "wandb.init(project=\"rag-datakit-finetunes\", name=\"all-minilm-l6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_args",
   "metadata": {},
   "source": [
    "## Configure Training Arguments\n",
    "\n",
    "We configure the training arguments for our embedding model. These parameters control various aspects of the training process including batch sizes, learning rate, precision settings, and checkpointing behavior. Key configurations include:\n",
    "\n",
    "- 5 training epochs with cosine learning rate scheduler\n",
    "- Mixed precision training using bf16 for efficiency\n",
    "- Gradient accumulation to achieve an effective batch size of 512\n",
    "- NO_DUPLICATES batch sampler to ensure diverse negative samples\n",
    "- Checkpointing at the end of each epoch with a limit of 3 saved models\n",
    "- Evaluation after each epoch to monitor validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efaba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=save_model_path,\n",
    "    num_train_epochs=5,                         # number of epochs\n",
    "    per_device_train_batch_size=32,             # train batch size\n",
    "    gradient_accumulation_steps=16,             # for a global batch size of 512\n",
    "    per_device_eval_batch_size=16,              # evaluation batch size\n",
    "    warmup_ratio=0.1,                           # warmup ratio\n",
    "    learning_rate=2e-5,                         # learning rate, 2e-5 is a good value\n",
    "    lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    tf32=True,                                  # use tf32 precision\n",
    "    bf16=True,                                  # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    eval_strategy=\"epoch\",                      # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",                      # save after each epoch\n",
    "    logging_steps=10,                           # log every 10 steps\n",
    "    save_total_limit=3,                         # save only the last 3 models\n",
    "    load_best_model_at_end=True,                # load the best model when training ends\n",
    "    report_to=\"wandb\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_init",
   "metadata": {},
   "source": [
    "## Initialize Model, Loss Function, and Trainer\n",
    "\n",
    "We initialize our sentence transformer model and configure the training components:\n",
    "\n",
    "- Load the base `all-minilm-l6-v2` model from sentence-transformers\n",
    "- Configure `MultipleNegativesRankingLoss` which is ideal for triplet training as it maximizes the similarity between anchor and positive pairs while minimizing similarity between anchor and negative pairs\n",
    "- Set up the `SentenceTransformerTrainer` with our model, training arguments, datasets, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07719cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050bb08a18864e29a85261a0f711015a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(model_id)\n",
    "train_loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['valid'],  \n",
    "    loss=train_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_execution",
   "metadata": {},
   "source": [
    "## Execute Model Training\n",
    "\n",
    "We start the training process using our configured trainer. The model will train for 5 epochs, with evaluation happening after each epoch. The training progress and metrics are tracked through Weights & Biases, showing both training and validation loss metrics.\n",
    "\n",
    "As training progresses, we can observe the validation loss decreasing, indicating that our model is learning to distinguish between semantically similar and dissimilar job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fade95f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.019654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.010993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.008991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.008272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.008129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.05468480090300242, metrics={'train_runtime': 20.756, 'train_samples_per_second': 363.269, 'train_steps_per_second': 0.723, 'total_flos': 0.0, 'train_loss': 0.05468480090300242, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_saving",
   "metadata": {},
   "source": [
    "## Save and Upload Trained Model\n",
    "\n",
    "After training is complete, we save the fine-tuned model to disk and upload it to Weights & Biases for versioning and sharing. The saved model includes all necessary components:\n",
    "\n",
    "- Model weights and configuration\n",
    "- Tokenizer files\n",
    "- Pooling layer configuration\n",
    "- Normalization components\n",
    "\n",
    "This ensures that the model can be easily loaded and used for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85cb1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c337c6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 16 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/vocab.txt',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/tokenizer_config.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/model.safetensors',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/sentence_bert_config.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/2_Normalize',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/README.md',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/tokenizer.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/checkpoint-9',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/config_sentence_transformers.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/checkpoint-12',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/config.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/training_args.bin',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/modules.json',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/1_Pooling',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/checkpoint-15',\n",
       " '/home/dnth/Desktop/rag-datakit/nbs/wandb/run-20250819_143233-qsq6z6xp/files/models/all-minilm-l6-v2/special_tokens_map.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "wandb.save(os.path.join(save_model_path, \"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Training Results and Next Steps\n",
    "\n",
    "The training completed successfully with a final validation loss of 0.00813, showing that our model has learned to effectively distinguish between semantically similar and dissimilar job descriptions. The Weights & Biases dashboard provides detailed metrics and visualizations of the training process.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To use this model in production:\n",
    "\n",
    "1. **Load the model** using `SentenceTransformer('./models/all-minilm-l6-v2')`\n",
    "2. **Evaluate** on a test set to verify performance on unseen data\n",
    "3. **Deploy** in your RAG pipeline for improved job description matching\n",
    "4. **Publish** to the Hugging Face Hub (uncomment the last cell) to share with the community\n",
    "\n",
    "The fine-tuned model is now ready to provide more accurate semantic similarity scores for job descriptions in your retrieval-augmented generation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655f61a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▂▁▁</td></tr><tr><td>eval/runtime</td><td>▃▇█▁▅</td></tr><tr><td>eval/samples_per_second</td><td>▅▂▁█▄</td></tr><tr><td>eval/steps_per_second</td><td>▅▂▁█▄</td></tr><tr><td>train/epoch</td><td>▁▃▅▅▆██</td></tr><tr><td>train/global_step</td><td>▁▃▅▅▆██</td></tr><tr><td>train/grad_norm</td><td>▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.00813</td></tr><tr><td>eval/runtime</td><td>0.4424</td></tr><tr><td>eval/samples_per_second</td><td>852.164</td></tr><tr><td>eval/steps_per_second</td><td>54.249</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>15</td></tr><tr><td>train/grad_norm</td><td>0.48494</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0675</td></tr><tr><td>train_loss</td><td>0.05468</td></tr><tr><td>train_runtime</td><td>20.756</td></tr><tr><td>train_samples_per_second</td><td>363.269</td></tr><tr><td>train_steps_per_second</td><td>0.723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">all-minilm-l6</strong> at: <a href='https://wandb.ai/dnth/rag-datakit-finetunes/runs/qsq6z6xp' target=\"_blank\">https://wandb.ai/dnth/rag-datakit-finetunes/runs/qsq6z6xp</a><br> View project at: <a href='https://wandb.ai/dnth/rag-datakit-finetunes' target=\"_blank\">https://wandb.ai/dnth/rag-datakit-finetunes</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 16 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250819_143233-qsq6z6xp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0349fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.push_to_hub(\"dnth/ssf-retriever-modernbert-embed-base\", exist_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-datakit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}